{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjGEL/bR/0tUbMGjrJM6oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minsung6333/AIFFEL_Quest/blob/main/%08%08deep_prj_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import matplotlib\n",
        "import seaborn\n",
        "import numpy\n",
        "import pandas\n",
        "import sklearn\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(seaborn.__version__)\n",
        "print(numpy.__version__)\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg7FpxninNB1",
        "outputId": "d5c74b26-9f18-42bc-e160-676a63321cc7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n",
            "3.7.1\n",
            "0.12.2\n",
            "1.23.5\n",
            "1.5.3\n",
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 진행방향\n",
        "---\n",
        "# 1. 모든 단어 사용\n",
        "\n",
        "`(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)`\n",
        "\n",
        "---\n",
        "# 2. 빈도수 상위 5,000개의 단어만 사용\n",
        "\n",
        "`(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)`\n",
        "\n",
        "---\n",
        "# 3. 직접 단어 개수를 설정해서 사용\n",
        "\n",
        "위 단계에서 5000으로 제시된 num_words를 다양하게 바꾸어 가며 성능을 확인해보세요. 변화된 단어 수에 따른 모델의 성능을 연구해 보세요. 최소 3가지 경우 이상을 실험해 보기를 권합니다.\n",
        "\n",
        "> 나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅\n",
        "\n",
        "# 4. 딥러닝 모델과 비교해 보기\n",
        "---\n",
        "위 과정을 통해 나온 최적의 모델과 단어 수 조건에서, 본인이 선택한 다른 모델을 적용한 결과와 비교해 봅시다. 감정 분석 등에 사용했던 RNN이나 1-D CNN 등의 딥러닝 모델 중 하나를 선택해서 오늘 사용했던 데이터셋을 학습해 보고 나오는 결과를 비교해 봅시다. 단, 공정한 비교를 위해 이때 Word2Vec 등의 pretrained model은 사용하지 않도록 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "sk3iX8g1pSO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가문항\t상세기준\n",
        "1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가?\n",
        "  - 3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.\n",
        "2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가?\n",
        "  - Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.\n",
        "3. 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가?\n",
        "  - 동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다."
      ],
      "metadata": {
        "id": "op8xRXypqiKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 목차\n"
      ],
      "metadata": {
        "id": "F5DaVX4tq3H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score #정확도 계산\n"
      ],
      "metadata": {
        "id": "zoI1rDXh7vPD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 호출 및 가공\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from tensorflow.keras.datasets import reuters\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "# 인덱스 수정을 위한 전처리\n",
        "index_to_word = { index+3 : word for word, index in word_index.items() }\n",
        "dtmvector = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "\n",
        "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index]=token\n",
        "print('=3')\n",
        "\n",
        "# 데이터 전처리를 위한 가공함수 생성\n",
        "def data_tfidf(num_words):\n",
        "  (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words= num_words, test_split=0.2)\n",
        "\n",
        "  # #decoding\n",
        "  decode_train = []\n",
        "  decode_test = []\n",
        "\n",
        "  for i in range(len(x_train)):\n",
        "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
        "    decode_train.append(t)\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    q = ' '.join([index_to_word[index] for index in x_test[i]])\n",
        "    decode_test.append(q)\n",
        "\n",
        "  x_train = decode_train\n",
        "  x_test = decode_test\n",
        "\n",
        "\n",
        "  x_train_dtm = dtmvector.fit_transform(x_train) #dtm 행렬 생성\n",
        "  tfidfv = tfidf_transformer.fit_transform(x_train_dtm) #tf-idf 행렬 생성\n",
        "\n",
        "  x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
        "  tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
        "\n",
        "  return x_train, y_train, x_test, y_test, tfidfv, tfidfv_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRT3QnWp4YvH",
        "outputId": "f9f1c668-4e06-47a4-9fc7-da7274c09245"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_num = 5000 # 단어 빈도수 설정\n",
        "\n",
        "x_train = data_tfidf(input_num)[0]\n",
        "y_train = data_tfidf(input_num)[1]\n",
        "x_test = data_tfidf(input_num)[2]\n",
        "y_test = data_tfidf(input_num)[3]\n",
        "tfidfv = data_tfidf(input_num)[4]\n",
        "tfidfv_test = data_tfidf(input_num)[5]"
      ],
      "metadata": {
        "id": "cE71j9v-533h"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidfv.shape)\n",
        "print(tfidfv_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k4Piqyk6BA4",
        "outputId": "b3ddb9a5-9580-46f9-db65-7e20a2e47da5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8982, 4867)\n",
            "(2246, 4867)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mb = MultinomialNB()\n",
        "cb = ComplementNB()\n",
        "lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000) #로지스틱 회귀\n",
        "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False) #선형 svm\n",
        "tree = DecisionTreeClassifier(max_depth=10, random_state=0) # dicision tree\n",
        "forest = RandomForestClassifier(n_estimators=5, random_state=0)#randomforest\n",
        "grbt = GradientBoostingClassifier(random_state=0) #gradientboosting verbose=3\n",
        "voting = VotingClassifier(estimators=[('lr', lr), ('cb', cb), ('grbt', grbt)], voting='soft') # voting"
      ],
      "metadata": {
        "id": "tKixRMfa8lrU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "ml_lst = [mb, cb, lr, lsvc, tree, forest, grbt, voting]\n",
        "\n",
        "precision = []\n",
        "recall = []\n",
        "f1_score = []\n",
        "\n",
        "\n",
        "for i in ml_lst:\n",
        "  print(str(i))\n",
        "  tqdm(i.fit(tfidfv, y_train))\n",
        "  predicted = i.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  # print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "  df = pd.DataFrame(classification_report(y_test, i.predict(tfidfv_test), zero_division=0, output_dict=True)).transpose()\n",
        "  precision.append(df['precision'].values.tolist())\n",
        "  recall.append(df['recall'].values.tolist())\n",
        "  f1_score.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rTSXKzHZ-CY",
        "outputId": "15a544a6-19de-4ef8-b1f5-6c635a2aa766"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplementNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=10000, max_iter=3000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC(C=1000, dual=False, max_iter=3000, penalty='l1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(max_depth=10, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(n_estimators=5, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoostingClassifier(random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000, max_iter=3000)),\n",
            "                             ('cb', ComplementNB()),\n",
            "                             ('grbt',\n",
            "                              GradientBoostingClassifier(random_state=0))],\n",
            "                 voting='soft')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL"
      ],
      "metadata": {
        "id": "XA9MSOmCwFWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "BPyLVA1ew2NO"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_num = 5000\n",
        "max_len = 100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=input_num, test_split=0.2)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "y_test_non = y_test\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_test_non"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqSvp7W486op",
        "outputId": "f7ae4223-edf0-42e5-c886-244b8e2981c3"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 10,  1, ...,  3,  3, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n",
        "\n",
        "model_5000 = Sequential()\n",
        "model_5000.add(Embedding(input_num, embedding_dim))\n",
        "model_5000.add(LSTM(hidden_units))\n",
        "model_5000.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_5000.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_5000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model_5000.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1b7gI3WIZfr",
        "outputId": "9364cf43-26cc-4e0e-c8a3-6df1b9c0f69c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 2.6268 - acc: 0.3527\n",
            "Epoch 1: val_acc improved from -inf to 0.46037, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 32s 388ms/step - loss: 2.6268 - acc: 0.3527 - val_loss: 2.2603 - val_acc: 0.4604\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - ETA: 0s - loss: 2.0620 - acc: 0.4869\n",
            "Epoch 2: val_acc improved from 0.46037 to 0.51959, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 33s 463ms/step - loss: 2.0620 - acc: 0.4869 - val_loss: 1.9424 - val_acc: 0.5196\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.8125 - acc: 0.5337\n",
            "Epoch 3: val_acc improved from 0.51959 to 0.55031, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 28s 392ms/step - loss: 1.8125 - acc: 0.5337 - val_loss: 1.7614 - val_acc: 0.5503\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.6952 - acc: 0.5626\n",
            "Epoch 4: val_acc improved from 0.55031 to 0.56812, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 38s 532ms/step - loss: 1.6952 - acc: 0.5626 - val_loss: 1.7007 - val_acc: 0.5681\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.5434 - acc: 0.6091\n",
            "Epoch 5: val_acc improved from 0.56812 to 0.60552, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 35s 494ms/step - loss: 1.5434 - acc: 0.6091 - val_loss: 1.6115 - val_acc: 0.6055\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.4316 - acc: 0.6335\n",
            "Epoch 6: val_acc did not improve from 0.60552\n",
            "71/71 [==============================] - 35s 492ms/step - loss: 1.4316 - acc: 0.6335 - val_loss: 1.6296 - val_acc: 0.5779\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.3491 - acc: 0.6500\n",
            "Epoch 7: val_acc improved from 0.60552 to 0.61754, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 36s 504ms/step - loss: 1.3491 - acc: 0.6500 - val_loss: 1.5081 - val_acc: 0.6175\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.1848 - acc: 0.6938\n",
            "Epoch 8: val_acc improved from 0.61754 to 0.64069, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 39s 557ms/step - loss: 1.1848 - acc: 0.6938 - val_loss: 1.4654 - val_acc: 0.6407\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.0701 - acc: 0.7204\n",
            "Epoch 9: val_acc improved from 0.64069 to 0.64648, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 30s 424ms/step - loss: 1.0701 - acc: 0.7204 - val_loss: 1.4079 - val_acc: 0.6465\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.9505 - acc: 0.7496\n",
            "Epoch 10: val_acc improved from 0.64648 to 0.65494, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 27s 381ms/step - loss: 0.9505 - acc: 0.7496 - val_loss: 1.4087 - val_acc: 0.6549\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.8605 - acc: 0.7769\n",
            "Epoch 11: val_acc improved from 0.65494 to 0.66696, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 27s 376ms/step - loss: 0.8605 - acc: 0.7769 - val_loss: 1.4324 - val_acc: 0.6670\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.7743 - acc: 0.7990\n",
            "Epoch 12: val_acc did not improve from 0.66696\n",
            "71/71 [==============================] - 28s 392ms/step - loss: 0.7743 - acc: 0.7990 - val_loss: 1.4282 - val_acc: 0.6589\n",
            "Epoch 13/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.6872 - acc: 0.8202\n",
            "Epoch 13: val_acc improved from 0.66696 to 0.67409, saving model to best_model_5000.h5\n",
            "71/71 [==============================] - 31s 433ms/step - loss: 0.6872 - acc: 0.8202 - val_loss: 1.4261 - val_acc: 0.6741\n",
            "Epoch 13: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "loaded_model = load_model('best_model_5000.h5') #model 호출\n",
        "predictions = model_5000.predict(X_test) #모델 예측\n",
        "pre_y = [np.argmax(predictions[i]) for i in range(len(predictions))] #y값과의 비교를 위한 softmax 후처리 진행\n",
        "df = pd.DataFrame(classification_report(y_test_non, np.array(pre_y), zero_division=0, output_dict=True)).transpose() #classification_report 생성"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kJlA7G9yH_2",
        "outputId": "148f6b47-f7c9-452d-a199-972653e92077"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 5s 64ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ml 결과값과의 비교를 위한 리스트 추가\n",
        "precision.append(df['precision'].values.tolist())\n",
        "recall.append(df['recall'].values.tolist())\n",
        "f1_score.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "id": "htGFuKBnyi5e"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdB78O3w0LKW",
        "outputId": "0be6975c-5329-4287-c45a-4cbb2178294a"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10000개 경우\n",
        "\n",
        "input_num = 10000\n",
        "\n",
        "x_train = data_tfidf(input_num)[0]\n",
        "y_train = data_tfidf(input_num)[1]\n",
        "x_test = data_tfidf(input_num)[2]\n",
        "y_test = data_tfidf(input_num)[3]\n",
        "tfidfv = data_tfidf(input_num)[4]\n",
        "tfidfv_test = data_tfidf(input_num)[5]"
      ],
      "metadata": {
        "id": "d_PJEcPF0Zsy"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "ml_lst = [mb, cb, lr, lsvc, tree, forest] # grbt, voting 시간관계상 두개 제외\n",
        "\n",
        "precision_10000 = []\n",
        "recall_10000 = []\n",
        "f1_score_10000 = []\n",
        "\n",
        "\n",
        "for i in ml_lst:\n",
        "  print(str(i))\n",
        "  tqdm(i.fit(tfidfv, y_train))\n",
        "  predicted = i.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  # print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "  df = pd.DataFrame(classification_report(y_test, i.predict(tfidfv_test), zero_division=0, output_dict=True)).transpose()\n",
        "  precision_10000.append(df['precision'].values.tolist())\n",
        "  recall_10000.append(df['recall'].values.tolist())\n",
        "  f1_score_10000.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoCwqSV80pKY",
        "outputId": "99c6f1f1-6254-45eb-a1ff-69196c5bfd6f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplementNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=10000, max_iter=3000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC(C=1000, dual=False, max_iter=3000, penalty='l1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(max_depth=10, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(n_estimators=5, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=input_num, test_split=0.2)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "Y_test_non = Y_test\n",
        "\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)"
      ],
      "metadata": {
        "id": "yZHL1bEm0924"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n",
        "\n",
        "model_10000 = Sequential()\n",
        "model_10000.add(Embedding(input_num, embedding_dim))\n",
        "model_10000.add(LSTM(hidden_units))\n",
        "model_10000.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_10000.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_10000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model_10000.fit(X_train, Y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUr5nD3J1OXl",
        "outputId": "c4756c95-1c6a-4ff8-ec39-7e75a7ced5cc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 2.6147 - acc: 0.3340\n",
            "Epoch 1: val_acc improved from -inf to 0.36866, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 31s 401ms/step - loss: 2.6147 - acc: 0.3340 - val_loss: 2.3305 - val_acc: 0.3687\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - ETA: 0s - loss: 1.9382 - acc: 0.4860\n",
            "Epoch 2: val_acc improved from 0.36866 to 0.52627, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 28s 392ms/step - loss: 1.9382 - acc: 0.4860 - val_loss: 1.7695 - val_acc: 0.5263\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.6839 - acc: 0.5459\n",
            "Epoch 3: val_acc improved from 0.52627 to 0.56456, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 27s 385ms/step - loss: 1.6839 - acc: 0.5459 - val_loss: 1.7095 - val_acc: 0.5646\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.5464 - acc: 0.5882\n",
            "Epoch 4: val_acc improved from 0.56456 to 0.59528, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 30s 429ms/step - loss: 1.5464 - acc: 0.5882 - val_loss: 1.6445 - val_acc: 0.5953\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.3342 - acc: 0.6431\n",
            "Epoch 5: val_acc improved from 0.59528 to 0.62689, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 27s 377ms/step - loss: 1.3342 - acc: 0.6431 - val_loss: 1.5202 - val_acc: 0.6269\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.1948 - acc: 0.6733\n",
            "Epoch 6: val_acc did not improve from 0.62689\n",
            "71/71 [==============================] - 27s 377ms/step - loss: 1.1948 - acc: 0.6733 - val_loss: 1.4865 - val_acc: 0.6189\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.0506 - acc: 0.7110\n",
            "Epoch 7: val_acc improved from 0.62689 to 0.65004, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 27s 383ms/step - loss: 1.0506 - acc: 0.7110 - val_loss: 1.4365 - val_acc: 0.6500\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.8917 - acc: 0.7582\n",
            "Epoch 8: val_acc improved from 0.65004 to 0.65672, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 28s 390ms/step - loss: 0.8917 - acc: 0.7582 - val_loss: 1.4589 - val_acc: 0.6567\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.7815 - acc: 0.7900\n",
            "Epoch 9: val_acc did not improve from 0.65672\n",
            "71/71 [==============================] - 28s 389ms/step - loss: 0.7815 - acc: 0.7900 - val_loss: 1.4553 - val_acc: 0.6545\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.6853 - acc: 0.8176\n",
            "Epoch 10: val_acc improved from 0.65672 to 0.68121, saving model to best_model_10000.h5\n",
            "71/71 [==============================] - 27s 383ms/step - loss: 0.6853 - acc: 0.8176 - val_loss: 1.4509 - val_acc: 0.6812\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.5957 - acc: 0.8418\n",
            "Epoch 11: val_acc did not improve from 0.68121\n",
            "71/71 [==============================] - 27s 382ms/step - loss: 0.5957 - acc: 0.8418 - val_loss: 1.5092 - val_acc: 0.6647\n",
            "Epoch 11: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "loaded_model = load_model('best_model_10000.h5') #model 호출\n",
        "predictions = model_10000.predict(X_test) #모델 예측\n",
        "pre_y = [np.argmax(predictions[i]) for i in range(len(predictions))] #y값과의 비교를 위한 softmax 후처리 진행\n",
        "df = pd.DataFrame(classification_report(y_test_non, np.array(pre_y), zero_division=0, output_dict=True)).transpose() #classification_report 생성"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBj04Gh21rSN",
        "outputId": "5cf37977-a713-4c0d-ff18-436fd33acb93"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 4s 45ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ml 결과값과의 비교를 위한 리스트 추가\n",
        "precision_10000.append(df['precision'].values.tolist())\n",
        "recall_10000.append(df['recall'].values.tolist())\n",
        "f1_score_10000.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "id": "LZy9PHLb1tvh"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(precision_10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iHG9d4W5T72",
        "outputId": "3b5638f3-676f-4c98-cc65-430eff8cdc98"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25000개 데이터 호출\n"
      ],
      "metadata": {
        "id": "NI-V8WtN5gRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_num = 25000\n",
        "\n",
        "x_train = data_tfidf(input_num)[0]\n",
        "y_train = data_tfidf(input_num)[1]\n",
        "x_test = data_tfidf(input_num)[2]\n",
        "y_test = data_tfidf(input_num)[3]\n",
        "tfidfv = data_tfidf(input_num)[4]\n",
        "tfidfv_test = data_tfidf(input_num)[5]"
      ],
      "metadata": {
        "id": "Z9iR8Dzm5fUc"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_lst = [mb, cb, lr, lsvc, tree, forest] # grbt, voting 시간관계상 두개 제외\n",
        "\n",
        "precision_25000 = []\n",
        "recall_25000 = []\n",
        "f1_score_25000 = []\n",
        "\n",
        "\n",
        "for i in ml_lst:\n",
        "  print(str(i))\n",
        "  tqdm(i.fit(tfidfv, y_train))\n",
        "  predicted = i.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  # print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "  df = pd.DataFrame(classification_report(y_test, i.predict(tfidfv_test), zero_division=0, output_dict=True)).transpose()\n",
        "  precision_25000.append(df['precision'].values.tolist())\n",
        "  recall_25000.append(df['recall'].values.tolist())\n",
        "  f1_score_25000.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E37bLttl5Z8L",
        "outputId": "7cf23223-e9f1-49cb-bbd3-dcb9d7b9668c"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplementNB()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=10000, max_iter=3000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC(C=1000, dual=False, max_iter=3000, penalty='l1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(max_depth=10, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier(n_estimators=5, random_state=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=input_num, test_split=0.2)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "Y_test_non = Y_test\n",
        "\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)"
      ],
      "metadata": {
        "id": "3n9hDKYb561Q"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n",
        "\n",
        "model_25000 = Sequential()\n",
        "model_25000.add(Embedding(input_num, embedding_dim))\n",
        "model_25000.add(LSTM(hidden_units))\n",
        "model_25000.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_25000.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_25000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model_25000.fit(X_train, Y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "supV52gh6JWU",
        "outputId": "8769968c-89b8-4c3a-ecc5-038a90167ea1"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 2.6081 - acc: 0.3378\n",
            "Epoch 1: val_acc improved from -inf to 0.46349, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 32s 423ms/step - loss: 2.6081 - acc: 0.3378 - val_loss: 2.1870 - val_acc: 0.4635\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - ETA: 0s - loss: 1.9609 - acc: 0.4842\n",
            "Epoch 2: val_acc improved from 0.46349 to 0.49911, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 30s 427ms/step - loss: 1.9609 - acc: 0.4842 - val_loss: 1.8196 - val_acc: 0.4991\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.7426 - acc: 0.5304\n",
            "Epoch 3: val_acc improved from 0.49911 to 0.55343, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 29s 414ms/step - loss: 1.7426 - acc: 0.5304 - val_loss: 1.7567 - val_acc: 0.5534\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.5641 - acc: 0.5891\n",
            "Epoch 4: val_acc improved from 0.55343 to 0.56901, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 28s 389ms/step - loss: 1.5641 - acc: 0.5891 - val_loss: 1.6478 - val_acc: 0.5690\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.3298 - acc: 0.6477\n",
            "Epoch 5: val_acc improved from 0.56901 to 0.61175, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 30s 418ms/step - loss: 1.3298 - acc: 0.6477 - val_loss: 1.5614 - val_acc: 0.6118\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 1.1881 - acc: 0.6788\n",
            "Epoch 6: val_acc did not improve from 0.61175\n",
            "71/71 [==============================] - 29s 408ms/step - loss: 1.1881 - acc: 0.6788 - val_loss: 1.6010 - val_acc: 0.5953\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.9887 - acc: 0.7381\n",
            "Epoch 7: val_acc improved from 0.61175 to 0.61309, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 30s 420ms/step - loss: 0.9887 - acc: 0.7381 - val_loss: 1.5521 - val_acc: 0.6131\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.8543 - acc: 0.7694\n",
            "Epoch 8: val_acc improved from 0.61309 to 0.64826, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 30s 427ms/step - loss: 0.8543 - acc: 0.7694 - val_loss: 1.5136 - val_acc: 0.6483\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.7042 - acc: 0.8143\n",
            "Epoch 9: val_acc did not improve from 0.64826\n",
            "71/71 [==============================] - 29s 409ms/step - loss: 0.7042 - acc: 0.8143 - val_loss: 1.5438 - val_acc: 0.6402\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.5956 - acc: 0.8399\n",
            "Epoch 10: val_acc did not improve from 0.64826\n",
            "71/71 [==============================] - 29s 415ms/step - loss: 0.5956 - acc: 0.8399 - val_loss: 1.6996 - val_acc: 0.6331\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.5198 - acc: 0.8645\n",
            "Epoch 11: val_acc improved from 0.64826 to 0.66118, saving model to best_model_25000.h5\n",
            "71/71 [==============================] - 31s 431ms/step - loss: 0.5198 - acc: 0.8645 - val_loss: 1.6183 - val_acc: 0.6612\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.4480 - acc: 0.8847\n",
            "Epoch 12: val_acc did not improve from 0.66118\n",
            "71/71 [==============================] - 30s 418ms/step - loss: 0.4480 - acc: 0.8847 - val_loss: 1.6635 - val_acc: 0.6572\n",
            "Epoch 12: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model_25000.h5') #model 호출\n",
        "predictions = model_25000.predict(X_test) #모델 예측\n",
        "pre_y = [np.argmax(predictions[i]) for i in range(len(predictions))] #y값과의 비교를 위한 softmax 후처리 진행\n",
        "df = pd.DataFrame(classification_report(y_test_non, np.array(pre_y), zero_division=0, output_dict=True)).transpose() #classification_report 생성"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbkpEk76u7g",
        "outputId": "d0149a1f-6e1f-4de2-aa80-71d808649eba"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 5s 66ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ml 결과값과의 비교를 위한 리스트 추가\n",
        "precision_25000.append(df['precision'].values.tolist())\n",
        "recall_25000.append(df['recall'].values.tolist())\n",
        "f1_score_25000.append(df['f1-score'].values.tolist())"
      ],
      "metadata": {
        "id": "YendyJnC6yfJ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과에 대한 해석은 아직 진행이 안되어 구글 스프레드 시트를 통하여 전달드리겠습니다.\n",
        "- [결과값에 대한 정리](https://docs.google.com/spreadsheets/d/13y-PEpo_4RUXDRrkGumSb9Lr2N5V-hvhPfOtxVaTNKM/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "yVtdGcE-Fah3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdLQOpjDAlSl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
